{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea0c14c",
   "metadata": {},
   "source": [
    "# Main Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d14e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Input\n",
    "from keras.layers import add\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c0839",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e207f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Courses language programming\\\\6_Deep Learning\\\\Image Caption Generator\\\\text-image caption\"\n",
    "token = \"caption.token.txt\"\n",
    "\n",
    "file_name = path + \"\\\\\" + token\n",
    "\n",
    "def Readfile(path):\n",
    "    file = open(path, \"r\")\n",
    "    info = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return info\n",
    "\n",
    "def Analysis(path):\n",
    "    data = Readfile(file_name)\n",
    "    data = data.split(\"\\n\")\n",
    "\n",
    "    discription = {}\n",
    "    for recoud in data:\n",
    "        img, caption = recoud.split(\"\\t\")\n",
    "        img = img[:-2]\n",
    "        if img in discription:\n",
    "            discription[img].append(caption)\n",
    "        else:\n",
    "            discription[img] = [caption]\n",
    "    \n",
    "    return discription\n",
    "\n",
    "data = Analysis(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6d7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(caption):    \n",
    "    punc = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    c = [word.lower() for word in caption.split() if (len(word) > 1) and (word.isalpha())]\n",
    "    c = [word.translate(punc) for word in c]\n",
    "    \n",
    "    return \" \".join(c)\n",
    "    \n",
    "def clean_text(data):\n",
    "    for image, caption in data.items():\n",
    "        for indx, caption in enumerate(caption):\n",
    "            data[image][indx] = preprocessing(caption)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "cleaning_data = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d43db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_resposity(data):\n",
    "    repostery_voc = set()\n",
    "\n",
    "    for img in data.keys():\n",
    "        [repostery_voc.update(caption.split()) for caption in data[img]]\n",
    "        \n",
    "    return repostery_voc\n",
    "\n",
    "\n",
    "def write_file(path, data):\n",
    "    lines = []\n",
    "    for img, caption in data.items():\n",
    "        for caption in caption:\n",
    "            lines.append(img + \"\\t\" + caption)\n",
    "    lines = \"\\n\".join(lines)\n",
    "    \n",
    "    file = open(path, 'w')\n",
    "    file.write(lines)\n",
    "    file.close()\n",
    "    \n",
    "write_file(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\cleaned_text.txt\", cleaning_data)\n",
    "\n",
    "rep_voc = generate_resposity(cleaning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a64a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8357"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rep_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70de835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_image = r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\images\"\n",
    "model = Xception(include_top = False, pooling = \"avg\")\n",
    "\n",
    "def feature_extraction(path, model):\n",
    "    features = {}\n",
    "    for imageName in os.listdir(path):\n",
    "        completa_path = image_file = path_image + \"\\\\\" + imageName\n",
    "        \n",
    "        image = Image.open(completa_path)\n",
    "\n",
    "        image = image.resize((299, 299))\n",
    "        image =  np.expand_dims(image, axis=0)\n",
    "\n",
    "        image = image / 127.5\n",
    "        image = image - 1\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[imageName] = feature\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01f07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature = feature_extraction(path_image, model)\n",
    "\n",
    "pickle.dump(feature, open(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\image_feature.bin\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d20c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pickle.load(open(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\image_feature.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b58159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\caption.trainImages.txt\"\n",
    "clean_token = r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\cleaned_text.txt\"\n",
    "feature_path = r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\image_feature.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad3e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    data = Readfile(path)\n",
    "    data = data.split(\"\\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_image = load_image(train_image_path)\n",
    "\n",
    "def load_token(path, images):\n",
    "    data = Readfile(path)\n",
    "    lines = data.split(\"\\n\")\n",
    "    \n",
    "    token = {}\n",
    "    for line in lines:\n",
    "        image, caption = line.split(\"\\t\")\n",
    "        if image in images:\n",
    "            if image not in token:\n",
    "                token[image] = []\n",
    "            token[image].append(\"<start> \" + caption + \" <end>\")\n",
    "            \n",
    "    return token\n",
    "            \n",
    "load_token = load_token(clean_token, train_image)\n",
    "\n",
    "def load_feature(path, images):\n",
    "    feature = pickle.load(open(path, \"rb\"))\n",
    "    selected_features = [{image:feature[image] for image in images if image in feature}]\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "train_feature = load_feature(feature_path, train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35257253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetsh_Data(data):\n",
    "    captions = []\n",
    "    for caps in data.values():\n",
    "        [captions.append(cap) for cap in caps]\n",
    "        \n",
    "    return captions\n",
    "def create_tokenizer(captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    return tokenizer\n",
    "\n",
    "def longenst_captions(caption):\n",
    "    return max(len(cap.split()) for cap in caption)\n",
    "\n",
    "captions = fetsh_Data(load_token)\n",
    "tokenizer = create_tokenizer(captions)\n",
    "voc_size = len(tokenizer.word_index) + 1\n",
    "max_cap = longenst_captions(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56e542cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequance(tokenizer, max_cap, voc_size, feature, caption):\n",
    "    input_1, input_2, output = [], [], []\n",
    "    for cap in captions:\n",
    "        seq = tokenizer.texts_to_sequences([cap])[0]\n",
    "        for index in range(len(seq)):\n",
    "            in_seq = seq[:index] \n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_cap, padding=\"post\")\n",
    "            output_w = seq[index]\n",
    "            output_w = to_categorical([output_w], num_classes=voc_size)[0]\n",
    "            input_1.append(feature)\n",
    "            input_2.append(in_seq)\n",
    "            output.append(output_w)\n",
    "    return np.array(input_1), np.array(input_2), np.array(output)\n",
    "\n",
    "def data_generator(tokenizer, feature, data, train_token, max_cap, voc_size):\n",
    "    while True:\n",
    "        for img, caption in data.items():\n",
    "            if img in feature:\n",
    "                f = feature[img][0]\n",
    "                input_img, input_seq, output_word = build_sequance(tokenizer, max_cap, voc_size, f, caption)\n",
    "                yield([[input_img, input_seq], output_word])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e41b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[in_img, in_seq], out_word] = next(data_generator(tokenizer, feature, data, load_token, max_cap, voc_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41ca3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_img.shape, in_seq.shape, out_word.shape\n",
    "\n",
    "feature_size = in_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cec1e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_model(num_features, longest_caption, output_size):\n",
    "    # CNN Model\n",
    "    input_img = Input(shape=(num_features,))\n",
    "    cnn_layer1 = Dropout(0.5)(input_img)\n",
    "    cnn_layer2 = Dense(256, activation=\"relu\")(cnn_layer1)\n",
    "    \n",
    "    # LSTM Model\n",
    "    input_seq = Input(shape=(longest_caption,))\n",
    "    lstm_layer1 = Embedding(output_size, 256, mask_zero=True)(input_seq)\n",
    "    lstm_layer2 = Dropout(0.5)(lstm_layer1)\n",
    "    lstm_layer3 = LSTM(256)(lstm_layer2)\n",
    "    \n",
    "    #Merging Model\n",
    "    merging_model = add([cnn_layer2, lstm_layer3])\n",
    "    final_model = Dense(256, activation=\"relu\")(merging_model)\n",
    "    \n",
    "    output = Dense(output_size, activation=\"softmax\")(final_model)\n",
    "    \n",
    "    model = Model(inputs = [input_img, input_seq], outputs=output)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentopy\", optimizer=\"adam\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c584098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Build_model(feature_size, max_cap, voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14bdfb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)       [(None, 33)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None, 2048)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 33, 256)              1856512   ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 2048)                 0         ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 33, 256)              0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 256)                  524544    ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 256)                  525312    ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 256)                  0         ['dense_8[0][0]',             \n",
      "                                                                     'lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 256)                  65792     ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 7252)                 1863764   ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4835924 (18.45 MB)\n",
      "Trainable params: 4835924 (18.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc728f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = len(load_token)\n",
    "\n",
    "# for i in range(16):\n",
    "#     generator = data_generator(tokenizer, feature, data, load_token, max_cap, voc_size)\n",
    "#     model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     model.save(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\"+str(i)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6e7c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(path, model):\n",
    "    try:    \n",
    "        image = Image.open(path)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        image = image.resize((299, 299))\n",
    "        image =  np.expand_dims(image, axis=0)\n",
    "        image = image / 127.5\n",
    "        image = image - 1.0\n",
    "        imgfeature = model.predict(image)\n",
    "        \n",
    "        return imgfeature\n",
    "    except:\n",
    "        print(\"Cannot read image\")\n",
    "        return None\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    return list(tokenizer.word_index)[index-1]\n",
    "\n",
    "def generate_caption(model, tokenizer, imgFeature, longenst_cap):\n",
    "    output_size = \"start\"\n",
    "    for i in range(longenst_cap):\n",
    "        seq = tokenizer.texts_to_sequences([output_size])[0]\n",
    "        seq = pad_sequences([seq], maxlen=longenst_cap)\n",
    "        \n",
    "        predict = model.predict([imgFeature, seq])\n",
    "        index = np.argmax(predict)\n",
    "        word = get_word(index, tokenizer)\n",
    "        \n",
    "        if word == \"end\":\n",
    "            break\n",
    "        output_seq += \" \"+word\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b5326da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\tokenizer\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ba7809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Xception(include_top=False, pooling=\"avg\")\n",
    "\n",
    "img_path = r\"\"\n",
    "\n",
    "cap_model = load_model(r\"\")\n",
    "\n",
    "tokenizer = pickle.load(open(r\"D:\\Courses language programming\\6_Deep Learning\\Image Caption Generator\\text-image caption\\tokenizer\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature = image_feature(img_path, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(cap_model, tokenizer, img_feature, max_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a440d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
